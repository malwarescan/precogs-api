# Code Samples: Streaming + Function Calling

Real-world examples for reference and comparison with our Precogs implementation.

---

## Example 1: Official OpenAI Node.js Example

**Source:** [openai-node repository](https://github.com/openai/openai-node) - `examples/function-call-stream.ts`

**Key Pattern:** Basic streaming + function calling with argument accumulation

```typescript
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const completion = await openai.chat.completions.create({
  model: "gpt-4-0613",
  messages: [
    { role: "user", content: "Get weather for NYC" }
  ],
  functions: [
    {
      name: "getWeather",
      description: "Returns weather for given city",
      parameters: {
        type: "object",
        properties: {
          city: { type: "string" }
        },
        required: ["city"]
      }
    }
  ],
  stream: true
});

let funcName = "";
let funcArgs = "";

for await (const chunk of completion) {
  const delta = chunk.choices[0].delta;
  
  // Accumulate function call name and arguments
  if (delta.function_call?.name) funcName += delta.function_call.name;
  if (delta.function_call?.arguments) funcArgs += delta.function_call.arguments;
  
  // Execute when complete
  if (chunk.choices[0].finish_reason === "function_call") {
    const args = JSON.parse(funcArgs);
    
    // Call the function
    const result = await getWeather(args.city);
    
    // Feed back to model
    const resp2 = await openai.chat.completions.create({
      model: "gpt-4-0613",
      messages: [
        ...previousMessages,
        { 
          role: "assistant", 
          content: "", 
          function_call: { name: funcName, arguments: args }
        },
        { 
          role: "function", 
          name: funcName, 
          content: JSON.stringify(result)
        }
      ]
    });
    
    // Stream follow-up response
    for await (const c2 of resp2) {
      process.stdout.write(c2.choices[0].delta.content || "");
    }
    break;
  }
  
  // Stream regular content
  process.stdout.write(delta.content || "");
}
```

**Key Insights:**
- âœ… Accumulates `funcName` and `funcArgs` across chunks
- âœ… Only executes when `finish_reason === "function_call"`
- âœ… Feeds result back to model for follow-up response
- âœ… Continues streaming after function execution

**How We Use This Pattern:**
- Our `openai-chat.js` lines 75-100: Same accumulation pattern
- Our `openai-chat.js` lines 112-180: Same execution and follow-up pattern

---

## Example 2: Community Discussion - Chunk Accumulation

**Source:** OpenAI Community Forum - "How to properly handle the function call in assistant streaming (Node.js)"

**Key Pattern:** Event-based handling with tool call deltas

```javascript
.on('toolCallDelta', toolCallDelta => {
  if (toolCallDelta.type === 'function') {
    // toolCallDelta.function.name
    // toolCallDelta.function.arguments
  }
});
```

**Key Insights:**
- Event-driven approach (alternative to our async generator)
- Emphasizes handling deltas incrementally
- Validates that arguments arrive in chunks

**How We Use This Pattern:**
- Our implementation uses async generator (more flexible)
- Same underlying concept: accumulate deltas until complete
- Our `openai-chat.js` lines 88-99: Handle deltas incrementally

---

## Example 3: SSE Streaming Pattern

**Source:** OpenFaaS blog - "Stream OpenAI responses from functions using Server Sent Events (SSE)"

**Key Pattern:** SSE formatting for browser streaming

```python
def stream():
  for chunk in chat_model.stream(prompt):
    yield f'data: {chunk.content}\n\n'
return Response(stream(), mimetype='text/event-stream')
```

**Key Insights:**
- SSE format: `data: {json}\n\n`
- Proper content type: `text/event-stream`
- Streaming generator pattern

**How We Use This Pattern:**
- Our `server.js` lines 459-464: SSE headers
- Our `server.js` line 493: SSE format `data: ${JSON.stringify(chunk)}\n\n`
- Our `server.js` lines 132-223: Full SSE endpoint implementation

---

## Example 4: Basic Streaming Mechanics

**Source:** January.sh - "Real-time OpenAI Response Streaming with Node.js"

**Key Pattern:** Simple streaming loop

```javascript
const completion = await openai.chat.completions.create({
  model: "gpt-4",
  stream: true,
  messages: [{ role: "user", content: "Once upon a time" }]
});

for await (const chunk of completion) {
  const content = chunk.choices[0].delta.content;
  process.stdout.write(content);
}
```

**Key Insights:**
- Basic streaming loop structure
- Access `delta.content` for text chunks
- Simple async iteration pattern

**How We Use This Pattern:**
- Our `openai-chat.js` lines 79-109: Same loop structure
- Our `openai-chat.js` line 103: Access `delta.content`
- Foundation for our more complex function-calling logic

---

## Example 5: Non-Streaming Function Calling

**Source:** NodeJS-OpenAI-Function-Calling-Example by YAV-AI

**Key Pattern:** Simple function calling (can be adapted with `stream: true`)

**Key Insights:**
- Shows function definition structure
- Demonstrates function execution flow
- Can be adapted by adding `stream: true` parameter

**How We Use This Pattern:**
- Our `invoke_precog.js`: Function definition structure
- Our `openai-chat.js` lines 65-73: Function definition in API call
- Our `callWithFunctionCallingSync()`: Non-streaming fallback version

---

## Comparison: Our Implementation vs Examples

| Pattern | Example | Our Code | Status |
|---------|---------|----------|--------|
| **Argument Accumulation** | Example 1: `funcArgs += delta.function_call.arguments` | `openai-chat.js` lines 97-98 | âœ… Matches |
| **Function Execution** | Example 1: Execute when `finish_reason === "function_call"` | `openai-chat.js` lines 112-134 | âœ… Matches |
| **Result Integration** | Example 1: Append `role: "function"` message | `openai-chat.js` lines 150-154 | âœ… Matches |
| **Follow-up Streaming** | Example 1: Continue streaming after function | `openai-chat.js` lines 164-180 | âœ… Matches |
| **SSE Format** | Example 3: `data: {json}\n\n` | `server.js` line 493 | âœ… Matches |
| **Streaming Loop** | Example 4: `for await (const chunk)` | `openai-chat.js` line 79 | âœ… Matches |
| **Function Definition** | Example 5: Schema structure | `invoke_precog.js` | âœ… Matches |

---

## How to Adapt These to Precogs

### 1. Use Example 1's Pattern

**What to Adapt:**
- âœ… Accumulate `function_call.name` + `function_call.arguments` across chunks
- âœ… Execute `invoke_precog` when `finish_reason === "function_call"`
- âœ… Feed function result back into conversation
- âœ… Continue streaming model response

**Our Implementation:**
- Already matches this pattern exactly
- See `src/integrations/openai-chat.js` lines 75-194

### 2. Use Example 3's SSE Pattern

**What to Adapt:**
- âœ… Stream each event to browser/CLI
- âœ… Use proper SSE headers
- âœ… Format as `data: {json}\n\n`

**Our Implementation:**
- Already matches this pattern
- See `server.js` lines 459-464, 493

### 3. Handle Edge Cases (Example 2)

**What to Adapt:**
- âœ… Document chunk-accumulation edge case
- âœ… Test split arguments
- âœ… Handle partial function calls

**Our Implementation:**
- Already handles this
- See `openai-chat.js` lines 88-99 (accumulation)
- See `openai-chat.js` lines 114-124 (error handling)

### 4. Combine with Job System

**What to Adapt:**
- âœ… Create job â†’ return `job_id` + URLs
- âœ… Stream job events via SSE/NDJSON
- âœ… Let worker process asynchronously

**Our Implementation:**
- Already implements this
- See `src/functions/invoke_precog.js` (job creation)
- See `server.js` lines 132-223 (SSE streaming)

---

## Key Learnings

### âœ… What We're Doing Right

1. **Argument Accumulation** - Correctly accumulating across chunks
2. **Function Execution** - Proper timing (only when `finish_reason === "function_call"`)
3. **Result Integration** - Feeding results back to model correctly
4. **SSE Format** - Proper headers and formatting
5. **Error Handling** - Handling parse errors and function failures

### ðŸ”§ Potential Improvements

1. **Event-Based Alternative** - Could consider event-driven approach (Example 2)
2. **Simpler Fallback** - Non-streaming version already exists (`callWithFunctionCallingSync`)
3. **More Examples** - Could add more edge case handling from community discussions

---

## Testing Checklist

Based on these examples, verify:

- [x] Arguments accumulate correctly across chunks
- [x] Function executes only when complete
- [x] Result fed back to model
- [x] Follow-up response streams correctly
- [x] SSE format correct (`data: {json}\n\n`)
- [x] Error handling for parse failures
- [x] Error handling for function execution failures
- [x] Client can consume SSE stream
- [x] Job creation works correctly
- [x] URLs returned correctly

---

## References

1. **Official Example:** [openai-node function-call-stream.ts](https://github.com/openai/openai-node/blob/main/examples/function-call-stream.ts)
2. **Community Discussion:** OpenAI Forum - Function call streaming
3. **SSE Pattern:** OpenFaaS blog - SSE streaming
4. **Basic Streaming:** January.sh - Real-time streaming
5. **Function Calling:** YAV-AI NodeJS-OpenAI-Function-Calling-Example

---

**Status:** Examples documented and compared  
**Last Updated:** $(date)

